{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "import pandas as pd\n",
    "import code.operators as ops\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bias = load('lr_loan_bias.joblib')\n",
    "lr = load('lr_loan.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    col_id  data_type      mean        std  min  max\n",
      "0  column1     binary       NaN        NaN  NaN  NaN\n",
      "1  column2     binary       NaN        NaN  NaN  NaN\n",
      "2  column3     binary       NaN        NaN  NaN  NaN\n",
      "3  column4     binary       NaN        NaN  NaN  NaN\n",
      "4  column5     binary       NaN        NaN  NaN  NaN\n",
      "5  column6     binary       NaN        NaN  NaN  NaN\n",
      "6  column7  numerical  0.344633  22.782046  NaN  NaN\n",
      "7  column8  numerical  0.019596  21.520722  NaN  NaN\n",
      "8  column9  numerical -0.163414  22.534570  NaN  NaN\n"
     ]
    }
   ],
   "source": [
    "mock = pd.read_csv('mock_input.csv')\n",
    "print(mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output(input, model, disadvantaged, request_id):\n",
    "    output = pd.DataFrame(columns=['request_id'])\n",
    "    output = output.set_index('request_id')\n",
    "    \n",
    "    #calculating disparity index and appending to output:\n",
    "    disparity = []\n",
    "    #simulation of 100 times since data is randomly normal distributed\n",
    "    for i in range(100):\n",
    "        #generating synthetic data based on user input:\n",
    "        X_new = ops.data_input(input)\n",
    "        #create prediction for synthetic data with uploaded model:\n",
    "        pred_n = ops.create_eval(model, X_new)\n",
    "        #calculate disparity index\n",
    "        disparity.append(ops.disparity_values(pred_n, 'Pred', 'column1', disadvantaged))\n",
    "    output.loc[request_id, 'disparity_index'] = statistics.mean(disparity)\n",
    "\n",
    "    mean_diff = []\n",
    "    for i in range(100):\n",
    "        X_new = ops.data_input(input)\n",
    "        pred_n = ops.create_eval(model, X_new)\n",
    "        #calculate mean difference\n",
    "        mean_diff.append(ops.mean_diff_values(pred_n, 'Pred', 'column1', disadvantaged))\n",
    "    output.loc[request_id, 'mean_difference'] = statistics.mean(mean_diff)\n",
    "    \n",
    "    equal_ops = []\n",
    "    for i in range(100):\n",
    "        X_new = ops.data_input(input)\n",
    "        pred_n = ops.create_eval(model, X_new)\n",
    "        #calculate equal opportunity index\n",
    "        equal_ops.append(ops.equal_ops_values(X_new,pred_n, 'Pred', 'column1', disadvantaged))\n",
    "    output.loc[request_id, 'equal_opportunity'] = statistics.mean(equal_ops)\n",
    "\n",
    "    positive = []\n",
    "    for i in range(100):\n",
    "        X_new = ops.data_input(input)\n",
    "        pred_n = ops.create_eval(model, X_new)\n",
    "        #calculate all positive predictions\n",
    "        positive.append(pred_n['Pred'].sum())\n",
    "    output.loc[request_id, 'positive_outcomes'] = statistics.mean(positive)/1000\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disparity_index</th>\n",
       "      <th>mean_difference</th>\n",
       "      <th>equal_opportunity</th>\n",
       "      <th>positive_outcomes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.02948</td>\n",
       "      <td>0.02123</td>\n",
       "      <td>0.031085</td>\n",
       "      <td>0.751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            disparity_index  mean_difference  equal_opportunity  \\\n",
       "request_id                                                        \n",
       "30                  0.02948          0.02123           0.031085   \n",
       "\n",
       "            positive_outcomes  \n",
       "request_id                     \n",
       "30                      0.751  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_output(mock, lr, 0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
