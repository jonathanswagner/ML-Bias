{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jona/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC,LinearSVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, classification_report \n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, roc_curve, auc\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "#from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAABICAYAAAAZFJRnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAACpklEQVR4nO3cv45NURjG4W/jiGiIGH8nNGhohMug0ijUp1C4CTU9jUqjUqkULoBCKE0jUQgyjWQSxSyNxhRkEsuas9/n6WZnF++XTPFL9mSm1loBAKTYN3oAAMD/JH4AgCjiBwCIIn4AgCjiBwCIIn4AgCgH/vbCNE3LqlpWVdXi0LU6tt570zBXFhujJ3T1fv+J0RO6ufz98+gJXW0vLo2e0FVbfBs9oavDB8+OntDVlx+boyd0c+zovH83P25dHD2hq80Pb7+21tZ2Pp92839+plMX23Tn4T8dtpdsrN8aPaGrC0fujZ7QzbtXD0ZP6GrrzMvRE7raPv109ISurp67P3pCV48+PR89oZvbN5+MntDV3XcvRk/o6tmNk29aa9d3PvfZCwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgCjiBwCIIn4AgChTa+3PL0zTsqqWv368UlXve48a6HhVfR09opM531blvlXnvtU159uq3LfqzrfW1nY+/Gv8/PbyNL1urV3/p7P2kDnfN+fbqty36ty3uuZ8W5X75spnLwAgivgBAKLsNn4ed1mxd8z5vjnfVuW+Vee+1TXn26rcN0u7+psfAIBV57MXABBF/AAAUcQPABBF/AAAUcQPABDlJ8VXdYPwIBCCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from code.models import cv_check, cv_bp, model_score, mod_eval, model_training\n",
    "from code.preprocessing import preprocess, prop_check, make_index, categorize, add_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv('loan_data_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(data_set)\n",
    "#prop_check(data_set)\n",
    "data_set = make_index(data_set)\n",
    "data_set = categorize(data_set)\n",
    "data_set.Loan_Status.replace({'Y': 1, 'N':0}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Adding new features:\n",
    "- naive estimation of monthly loan return (LoanAmount/Loan_Amount_Term) normalized and with ln() let us get the distribution as closer as can be to normal distribution\n",
    "- total income (ApplicantIncome + CoaplicantIncome) normlized and with ln() let us get the distribution as closer as can be to normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = add_feat(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lets check the new features distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_plt(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the skew and the kurtosis are much closer to 0, as expected from normalized normal distribution\n",
    "- now lets drop the neglectable features those are dependeds of the new features created above\n",
    "- i found in previous runs the Married and Dependents are neglectable too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropit=['LoanAmount', \n",
    "        'Loan_Amount_Term', \n",
    "        'ApplicantIncome',\n",
    "        'CoapplicantIncome',\n",
    "        'Married',\n",
    "        'Dependents_0',\n",
    "        'Dependents_1',\n",
    "        'Dependents_2']\n",
    "data_set.drop(columns=dropit, \n",
    "           inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the following is a sanity check for the distribution between Y and N in the target column\n",
    "- the distribution between the values didn't change much (less than 0.5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['Loan_Status'].value_counts(normalize=True)\n",
    "#sns.barplot(data = data_set.Loan_Status.value_counts(normalize = True).reset_index(),\n",
    "#            x = 'index',\n",
    "#            y = 'Loan_Status',\n",
    "#            palette=current_palette)\n",
    "#plt.grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In our case: binary classification, the \"cross_val_score\" function uses StratifiedKFold cross validation to reduce the bias effect in imbalanced data.\n",
    "- it is a good approach for the cases the target column distribution is biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1,1,figsize= (20,8),squeeze=False, sharey=True)\n",
    "cv_bp(cv_check(data_set.drop(['Loan_Status'],axis=1),\n",
    "               data_set.Loan_Status,10), '{} without NAs'.format('train'),axes[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using grid search cross validation we scanned to find the optimal values for each model variables\n",
    "- the values chosen after several runs to get the optimum variables within the optimal range for the roc_auc as closer we can get to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_values = {'max_features': [4, 5, 6, 7],\n",
    "              'max_depth': [3, 7, 11, 13]}\n",
    "scorers_list = ['accuracy','roc_auc','precision','recall', 'f1']\n",
    "\n",
    "rf_cv = model_score(data_set,\n",
    "            RandomForestClassifier(random_state=42, \n",
    "                                   n_jobs=4, \n",
    "                                   class_weight='balanced_subsample', \n",
    "                                   n_estimators=50), \n",
    "            grid_values, \n",
    "            scorers_list)\n",
    "\n",
    "temp_df1 = pd.DataFrame()\n",
    "for i in scorers_list:\n",
    "      temp_df1[i]=rf_cv[i].cv_results_['mean_test_score'][rf_cv[i].cv_results_['param_max_features']==4]\n",
    "temp_df1['max_depth'] = rf_cv['roc_auc'].cv_results_['param_max_depth'][rf_cv['roc_auc'].cv_results_['param_max_features']==4]\n",
    "temp_df1.set_index('max_depth', inplace=True)\n",
    "print('4:\\n')\n",
    "temp_df1\n",
    "\n",
    "temp_df2 = pd.DataFrame()\n",
    "for i in scorers_list:\n",
    "      temp_df2[i]=rf_cv[i].cv_results_['mean_test_score'][rf_cv[i].cv_results_['param_max_features']==6]\n",
    "temp_df2['max_depth'] = rf_cv['roc_auc'].cv_results_['param_max_depth'][rf_cv['roc_auc'].cv_results_['param_max_features']==6]\n",
    "temp_df2.set_index('max_depth', inplace=True)\n",
    "print('6:\\n')\n",
    "temp_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_values = {'C': [0.01, 0.1, 1, 10, 100],\n",
    "              'penalty': ['l1', 'l2']}\n",
    "scorers_list = ['accuracy','roc_auc','precision','recall', 'f1']\n",
    "\n",
    "\n",
    "lr_cv = model_score(data_set,\n",
    "                    LogisticRegression(solver='liblinear',random_state=42, max_iter=500,\n",
    "                                      class_weight='balanced'),\n",
    "                    grid_values,\n",
    "                    scorers_list)\n",
    "\n",
    "\n",
    "temp_df1 = pd.DataFrame()\n",
    "for i in scorers_list:\n",
    "      temp_df1[i]=lr_cv[i].cv_results_['mean_test_score'][lr_cv[i].cv_results_['param_penalty']=='l1']\n",
    "temp_df1['C'] = lr_cv['roc_auc'].cv_results_['param_C'][lr_cv['roc_auc'].cv_results_['param_penalty']=='l1']\n",
    "temp_df1.set_index('C', inplace=True)\n",
    "print('l1:\\n')\n",
    "temp_df1\n",
    "\n",
    "temp_df2 = pd.DataFrame()\n",
    "for i in scorers_list:\n",
    "      temp_df2[i]=lr_cv[i].cv_results_['mean_test_score'][lr_cv[i].cv_results_['param_penalty']=='l2']\n",
    "temp_df2['C'] = lr_cv['roc_auc'].cv_results_['param_C'][lr_cv['roc_auc'].cv_results_['param_penalty']=='l2']\n",
    "temp_df2.set_index('C', inplace=True)\n",
    "print('l2:\\n')\n",
    "temp_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_values = {'C': [1, 10],\n",
    "              'gamma': [0.5, 0.7, 0.9, 0.95]}\n",
    "scorers_list = ['accuracy','roc_auc','precision','recall', 'f1']\n",
    "\n",
    "\n",
    "svc_cv = model_score(data_set,\n",
    "                    SVC(random_state=42, class_weight='balanced',kernel='rbf'),\n",
    "                    grid_values,\n",
    "                    scorers_list)\n",
    "\n",
    "\n",
    "temp_df1 = pd.DataFrame()\n",
    "for i in scorers_list:\n",
    "      temp_df1[i]=svc_cv[i].cv_results_['mean_test_score'][svc_cv[i].cv_results_['param_C']==1]\n",
    "temp_df1['gamma'] = svc_cv['roc_auc'].cv_results_['param_gamma'][svc_cv['roc_auc'].cv_results_['param_C']==1]\n",
    "temp_df1.set_index('gamma', inplace=True)\n",
    "print('C=1:\\n')\n",
    "temp_df1\n",
    "\n",
    "temp_df2 = pd.DataFrame()\n",
    "for i in scorers_list:\n",
    "      temp_df2[i]=svc_cv[i].cv_results_['mean_test_score'][svc_cv[i].cv_results_['param_C']==10]\n",
    "temp_df2['gamma'] = svc_cv['roc_auc'].cv_results_['param_gamma'][svc_cv['roc_auc'].cv_results_['param_C']==10]\n",
    "temp_df2.set_index('gamma', inplace=True)\n",
    "print('C=10:\\n')\n",
    "temp_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen values per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "max_depth=11\n",
    "max_features=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression\n",
    "lr_C=0.1\n",
    "penalty='l1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "svc_C=1\n",
    "gamma=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 0.333\n",
    "rf = model_training(RandomForestClassifier(random_state=42, \n",
    "                                           n_jobs=4, \n",
    "                                           n_estimators=50, \n",
    "                                           max_depth=max_depth,\n",
    "                                           max_features=max_features),data_set, ts)\n",
    "\n",
    "t=data_set.drop(columns=['Loan_Status'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(t,\n",
    "                                                     data_set['Loan_Status'],\n",
    "                                                     test_size=.333,\n",
    "                                                     stratify=data_set['Loan_Status'])\n",
    "\n",
    "mod_eval(data_set, rf.predict(X_test), rf.predict_proba(X_test), y_test, 'RandomForest')\n",
    "fi_df = pd.DataFrame({'fi': rf.feature_importances_},index=t.columns).sort_values(by='fi', ascending=False)\n",
    "X_train.to_csv('x_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = model_training(LogisticRegression(C=lr_C, \n",
    "                                       penalty=penalty,\n",
    "                                       solver='liblinear',\n",
    "                                       max_iter=1000),data_set)\n",
    "\n",
    "t=data_set.drop(columns=['Loan_Status'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(t,\n",
    "                                                    data_set['Loan_Status'],\n",
    "                                                    test_size=ts,\n",
    "                                                    random_state = 42, stratify=data_set['Loan_Status'])\n",
    "\n",
    "t = 0.71\n",
    "predprob = lr.predict_proba(X_test)\n",
    "\n",
    "pred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]\n",
    "\n",
    "#pred_y = lr.predict(X_test)\n",
    "mod_eval(data_set, pred_y, lr.predict_proba(X_test), y_test, 'LogisticRegressin') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = model_training(GaussianNB(),data_set)\n",
    "\n",
    "t=data_set.drop(columns=['Loan_Status'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(t,\n",
    "                                                    data_set['Loan_Status'],\n",
    "                                                    test_size=ts,\n",
    "                                                    random_state = 42, stratify=data_set['Loan_Status'])\n",
    "\n",
    "\n",
    "t = 0.75\n",
    "predprob = gnb.predict_proba(X_test)\n",
    "\n",
    "pred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]\n",
    "#pred_y = gnb.predict(X_test)\n",
    "mod_eval(data_set,pred_y, gnb.predict_proba(X_test), y_test, 'GaussianNB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = model_training(SVC(kernel='linear',\n",
    "                         C=1, \n",
    "                         gamma='auto',\n",
    "                         class_weight='balanced',\n",
    "                         probability=True),data_set)\n",
    "t=data_set.drop(columns=['Loan_Status'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(t,\n",
    "                                                    data_set['Loan_Status'],\n",
    "                                                    test_size=ts,\n",
    "                                                    random_state = 42, stratify=data_set['Loan_Status'])\n",
    "\n",
    "t=0.75\n",
    "predprob = svc.predict_proba(X_test)\n",
    "pred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export preTrained models\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "dump(svc, 'svc_loan.joblib')\n",
    "dump(rf, 'rf_loan.joblib')\n",
    "dump(gnb, 'gnb_loan.joblib')\n",
    "dump(lr, 'lr_loan.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import GermanDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick demonstration of disparate impact using a built in dataset\n",
    "\n",
    "dataset_orig = GermanDataset(\n",
    "    protected_attribute_names=['age'],           # this dataset also contains protected\n",
    "                                                 # attribute for \"sex\" which we do not\n",
    "                                                 # consider in this evaluation\n",
    "    privileged_classes=[lambda x: x >= 25],      # age >=25 is considered privileged\n",
    "    features_to_drop=['personal_status', 'sex'] # ignore sex-related attributes\n",
    ")\n",
    "\n",
    "#dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#back to the initial loan application dataset\n",
    "\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from: https://www.ambiata.com/blog/2019-12-13-bias-detection-and-mitigation/\n",
    "\n",
    "def get_disparity_index(di):\n",
    "    return 1 - np.minimum(di, 1 / di)\n",
    "\n",
    "df_aif = BinaryLabelDataset(df=data_set, label_names=['Loan_Status'], protected_attribute_names=['Gender'])\n",
    "\n",
    "privileged_group = [{'Gender': 1}] #male=1\n",
    "unprivileged_group = [{'Gender': 0}] #female=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig = BinaryLabelDatasetMetric(df_aif, unprivileged_group, privileged_group)\n",
    "print('1-min(DI, 1/DI):', get_disparity_index(metric_orig.disparate_impact()).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the probability of success given the jobseeker is unprivileged (female), divided by the probability of success given the jobseeker is privileged (male). We further recast this as 1 - min(DI, 1/DI), since DI can be greater than 1, which would mean that the privileged group is disadvantaged. For our fairness benchmark, we require that 1 - min(DI, 1/DI) < 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_disparity_index(metric_orig.disparate_impact()).round(3) < 0.2:\n",
    "    print('The original data can be considered to be not biased')\n",
    "else:\n",
    "    print('There is a potential bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the predicted results of the previously defined test data to see if the model output was biased when feeding it test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pred_y, columns=['Pred'])\n",
    "pred_test = pd.concat([X_test.reset_index(drop='True'),df.reset_index(drop='True')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aif = BinaryLabelDataset(df=pred_test, label_names=['Pred'], protected_attribute_names=['Gender'])\n",
    "\n",
    "metric_orig = BinaryLabelDatasetMetric(df_aif, unprivileged_group, privileged_group)\n",
    "print('1-min(DI, 1/DI):', get_disparity_index(metric_orig.disparate_impact()).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_disparity_index(metric_orig.disparate_impact()).round(3) < 0.2:\n",
    "    print('The outcome of the test data can be considered to be not biased')\n",
    "else:\n",
    "    print('There is a potential bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, random data is generated to see if the pre-trained model shows a bias on completely unbiased data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_binary(series, allow_na=False):\n",
    "    if allow_na:\n",
    "        series.dropna(inplace=True)\n",
    "    return sorted(series.unique()) == [0, 1]\n",
    "\n",
    "X_new = pd.DataFrame(columns=[])\n",
    "\n",
    "for c in X_test.columns:\n",
    "    if is_binary(X_test[c]):\n",
    "        X_new[c] = np.random.binomial(1, .5, 1000)\n",
    "    else:\n",
    "        X_new[c] = np.random.normal(X_test[c].describe()[1], X_test[c].describe()[2], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the loan application outcome with the normally distributed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_y_n = svc.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n = pd.DataFrame(pred_y_n, columns=['Pred'])\n",
    "pred_n = pd.concat([X_new.reset_index(drop='True'),df_n.reset_index(drop='True')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aif = BinaryLabelDataset(df=pred_n, label_names=['Pred'], protected_attribute_names=['Gender'])\n",
    "\n",
    "metric_orig = BinaryLabelDatasetMetric(df_aif, unprivileged_group, privileged_group)\n",
    "print('1-min(DI, 1/DI):', get_disparity_index(metric_orig.disparate_impact()).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_disparity_index(metric_orig.disparate_impact()).round(3) < 0.2:\n",
    "    print('The outcome of the test data can be considered to be not biased')\n",
    "else:\n",
    "    print('There is a potential bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
